# Local LLM Django Chat ğŸ¤–

A feature-rich Django web application for chatting with locally-hosted LLMs via Ollama, featuring **multi-mode AI interactions**, user management, and pricing tiers.

![Version](https://img.shields.io/badge/version-2.0-blue)
![Python](https://img.shields.io/badge/python-3.10+-green)
![Django](https://img.shields.io/badge/django-5.2.5-darkgreen)
![License](https://img.shields.io/badge/license-MIT-orange)

## âœ¨ Features

### ğŸ­ Multi-Mode AI Interactions
- **ğŸ’¬ Normal Mode**: Standard conversational AI
- **ğŸ“ Teacher Mode**: Pedagogical, student-friendly explanations
- **ğŸ”¬ Researcher Mode**: Academic, evidence-based responses
- **ğŸ›ï¸ Council Mode**: Multi-model debate and synthesis (queries 2-3 models simultaneously)

### ğŸ” User Management
- Registration & Login system
- User-specific chat history
- Session persistence

### ğŸ’ Pricing Tiers
- **Free**: Access to 3B and smaller models
- **Pro ($19/mo)**: Access to 7B+ models

### ğŸ’¬ Chat Features
- Real-time streaming responses
- Dynamic model selection (auto-detects installed Ollama models)
- Conversation history with timestamps
- Dark-mode premium UI

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- [Ollama](https://ollama.com) installed
- At least one LLM model (e.g., `ollama pull llama3.2`)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/local-llm-django.git
cd local-llm-django
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Run migrations**
```bash
python manage.py migrate
```

4. **Start Ollama** (in a separate terminal)
```bash
ollama serve
```

5. **Run the application**
```bash
python manage.py runserver
```

Or use the automated script:
```powershell
.\start_app.ps1
```

6. **Access the app**
Navigate to `http://127.0.0.1:8000/`

## ğŸ¯ Usage

1. **Register**: Create a new account (default: Free tier)
2. **Select Mode**: Choose from Normal, Teacher, Researcher, or Council
3. **Select Model**: Pick from your installed Ollama models
4. **Chat**: Start asking questions!

### Council Mode
Council Mode is the standout feature - it queries multiple models in parallel and synthesizes their responses:
- Requires 2+ models installed
- 3-5x slower than normal mode
- Shows individual model responses before synthesis

## ğŸ“ Project Structure

```
LocalLLM/
â”œâ”€â”€ chat/                   # Main chat app
â”‚   â”œâ”€â”€ models.py          # Database models (UserProfile, Conversation, Message)
â”‚   â”œâ”€â”€ views.py           # Backend logic with mode-specific prompts
â”‚   â”œâ”€â”€ templates/         # HTML templates
â”‚   â””â”€â”€ static/            # CSS/JS
â”œâ”€â”€ local_llm/             # Django project settings
â”œâ”€â”€ manage.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ start_app.ps1          # Automated startup script
â””â”€â”€ README.md
```

## ğŸ› ï¸ Technologies

- **Backend**: Django 5.2.5, Python 3.13
- **AI**: Ollama Python SDK
- **Frontend**: Vanilla JavaScript, CSS (Dark Mode)
- **Database**: SQLite (easily swappable)
- **Auth**: Django built-in authentication

## ğŸ”§ Configuration

Environment variables (`.env`):
```env
DEBUG=True
SECRET_KEY=your-secret-key
OLLAMA_API_URL=http://127.0.0.1:11434
```

## ğŸ³ Docker Deployment

```bash
docker-compose up
```

The `docker-compose.yml` includes both Django and Ollama services with GPU support.

## ğŸ“ Mode Details

### Teacher Mode ğŸ“
System Prompt: *"You are a patient and experienced teacher..."*
- Simplifies complex concepts
- Uses analogies and examples
- Step-by-step explanations

### Researcher Mode ğŸ”¬
System Prompt: *"You are an academic researcher..."*
- Evidence-based responses
- Multiple perspectives
- Research paper structure

### Council Mode ğŸ›ï¸
- Queries top 3 installed models
- Streams each model's response
- Synthesizes final answer
- Gracefully handles incompatible models

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.com) for local LLM hosting
- Django community
- All contributors

## ğŸ“§ Contact

For questions or suggestions, please open an issue on GitHub.

---

**Built with â¤ï¸ for the local AI community**
